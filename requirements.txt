# Context Distillation - Requirements
# Research: On-policy vs off-policy context distillation

# Core ML
torch>=2.0.0
transformers>=4.35.0
peft>=0.7.0
datasets>=2.14.0

# Training
accelerate>=0.25.0
bitsandbytes>=0.41.0
wandb>=0.16.0
tensorboard>=2.15.0

# Tinker SDK (official API client)
tinker>=0.7.0
tinker-cookbook>=0.1.0

# HTTP client
httpx>=0.25.0

# Evaluation
evaluate>=0.4.0
scikit-learn>=1.3.0
sentence-transformers>=2.2.0

# Data processing
numpy>=1.24.0
tqdm>=4.65.0

# Utilities
python-dotenv>=1.0.0
